{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericae9/Autonomous-Vehicle-Object-Detection/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphJhxywc20W"
      },
      "source": [
        "This notebook contains the code to fine tune the [PyTorch Faster-RCNN model](http://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUGVn_aPuTKK"
      },
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFzs9Pyqcz_Q"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ98f398tYP3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SF1R3y0tjK6"
      },
      "source": [
        "import os\n",
        "# Edit the file path below to go to the folder containing the training, val, and\n",
        "# test images folders.\n",
        "BASE_PATH = '/content/drive/My Drive/CV_Project/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVCa8AcJed_a"
      },
      "source": [
        "Before running the cell below, place the file [pt_util.py](https://gist.github.com/pjreddie/e531394d779af2da9201096af0dba78a) from [CSE 543 Deep Learning](https://github.com/pjreddie/uwnet) in the folder defined by `BASE_PATH`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAkH3ave0RhW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "import h5py\n",
        "import sys\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7lI3OpaFUl"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RehXXdvkdPkd"
      },
      "source": [
        "## Create helper functions to save and load the model, which are from [CSE 543 Deep Learning](https://github.com/pjreddie/uwnet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHf7mVcIvBfD"
      },
      "source": [
        "# The functions in this cell are from CSE 543 Deep Learning.\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    # For 2.7\n",
        "    import cPickle as pickle\n",
        "except:\n",
        "    # For 3.x\n",
        "    import pickle\n",
        "\n",
        "\n",
        "def restore(net, save_file):\n",
        "    \"\"\"Restores the weights from a saved file\n",
        "\n",
        "    This does more than the simple Pytorch restore. It checks that the names\n",
        "    of variables match, and if they don't doesn't throw a fit. It is similar\n",
        "    to how Caffe acts. This is especially useful if you decide to change your\n",
        "    network architecture but don't want to retrain from scratch.\n",
        "\n",
        "    Args:\n",
        "        net(torch.nn.Module): The net to restore\n",
        "        save_file(str): The file path\n",
        "    \"\"\"\n",
        "\n",
        "    net_state_dict = net.state_dict()\n",
        "    restore_state_dict = torch.load(save_file)\n",
        "\n",
        "    restored_var_names = set()\n",
        "\n",
        "    print('Restoring:')\n",
        "    for var_name in restore_state_dict.keys():\n",
        "        if var_name in net_state_dict:\n",
        "            var_size = net_state_dict[var_name].size()\n",
        "            restore_size = restore_state_dict[var_name].size()\n",
        "            if var_size != restore_size:\n",
        "                print('Shape mismatch for var', var_name, 'expected', var_size, 'got', restore_size)\n",
        "            else:\n",
        "                if isinstance(net_state_dict[var_name], torch.nn.Parameter):\n",
        "                    # backwards compatibility for serialized parameters\n",
        "                    net_state_dict[var_name] = restore_state_dict[var_name].data\n",
        "                try:\n",
        "                    net_state_dict[var_name].copy_(restore_state_dict[var_name])\n",
        "                    print(str(var_name) + ' -> \\t' + str(var_size) + ' = ' + str(int(np.prod(var_size) * 4 / 10**6)) + 'MB')\n",
        "                    restored_var_names.add(var_name)\n",
        "                except Exception as ex:\n",
        "                    print('While copying the parameter named {}, whose dimensions in the model are'\n",
        "                          ' {} and whose dimensions in the checkpoint are {}, ...'.format(\n",
        "                              var_name, var_size, restore_size))\n",
        "                    raise ex\n",
        "\n",
        "    ignored_var_names = sorted(list(set(restore_state_dict.keys()) - restored_var_names))\n",
        "    unset_var_names = sorted(list(set(net_state_dict.keys()) - restored_var_names))\n",
        "    print('')\n",
        "    if len(ignored_var_names) == 0:\n",
        "        print('Restored all variables')\n",
        "    else:\n",
        "        print('Did not restore:\\n\\t' + '\\n\\t'.join(ignored_var_names))\n",
        "    if len(unset_var_names) == 0:\n",
        "        print('No new variables')\n",
        "    else:\n",
        "        print('Initialized but did not modify:\\n\\t' + '\\n\\t'.join(unset_var_names))\n",
        "\n",
        "    print('Restored %s' % save_file)\n",
        "\n",
        "\n",
        "def restore_latest(net, folder):\n",
        "    \"\"\"Restores the most recent weights in a folder\n",
        "\n",
        "    Args:\n",
        "        net(torch.nn.module): The net to restore\n",
        "        folder(str): The folder path\n",
        "    Returns:\n",
        "        int: Attempts to parse the epoch from the state and returns it if possible. Otherwise returns 0.\n",
        "    \"\"\"\n",
        "\n",
        "    checkpoints = sorted(glob.glob(folder + '/*.pt'), key=os.path.getmtime)\n",
        "    start_it = 0\n",
        "    if len(checkpoints) > 0:\n",
        "        restore(net, checkpoints[-1])\n",
        "        try:\n",
        "            start_it = int(re.findall(r'\\d+', checkpoints[-1])[-1])\n",
        "        except:\n",
        "            pass\n",
        "    return start_it\n",
        "\n",
        "\n",
        "def save(net, file_name, num_to_keep=1):\n",
        "    \"\"\"Saves the net to file, creating folder paths if necessary.\n",
        "\n",
        "    Args:\n",
        "        net(torch.nn.module): The network to save\n",
        "        file_name(str): the path to save the file.\n",
        "        num_to_keep(int): Specifies how many previous saved states to keep once this one has been saved.\n",
        "            Defaults to 1. Specifying < 0 will not remove any previous saves.\n",
        "    \"\"\"\n",
        "\n",
        "    folder = os.path.dirname(file_name)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    torch.save(net.state_dict(), file_name)\n",
        "    extension = os.path.splitext(file_name)[1]\n",
        "    checkpoints = sorted(glob.glob(folder + '/*' + extension), key=os.path.getmtime)\n",
        "    print('Saved %s\\n' % file_name)\n",
        "    if num_to_keep > 0:\n",
        "        for ff in checkpoints[:-num_to_keep]:\n",
        "            os.remove(ff)\n",
        "\n",
        "def write_log(filename, data):\n",
        "    \"\"\"Pickles and writes data to a file\n",
        "\n",
        "    Args:\n",
        "        filename(str): File name\n",
        "        data(pickleable object): Data to save\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(filename)):\n",
        "        os.makedirs(os.path.dirname(filename))\n",
        "    pickle.dump(data, open(filename, 'wb'))\n",
        "\n",
        "def read_log(filename, default_value=None):\n",
        "    \"\"\"Reads pickled data or returns the default value if none found\n",
        "\n",
        "    Args:\n",
        "        filename(str): File name\n",
        "        default_value(anything): Value to return if no file is found\n",
        "    Returns:\n",
        "        unpickled file\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        return pickle.load(open(filename, 'rb'))\n",
        "    return default_value\n",
        "\n",
        "def show_images(images, titles=None, columns=5, max_rows=5):\n",
        "    \"\"\"Shows images in a tiled format\n",
        "\n",
        "    Args:\n",
        "        images(list[np.array]): Images to show\n",
        "        titles(list[string]): Titles for each of the images\n",
        "        columns(int): How many columns to use in the tiling\n",
        "        max_rows(int): If there are more than columns * max_rows images, only the first n of them will be shown.\n",
        "    \"\"\"\n",
        "\n",
        "    images = images[:min(len(images), max_rows * columns)]\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for ii, image in enumerate(images):\n",
        "        plt.subplot(len(images) / columns + 1, columns, ii + 1)\n",
        "        plt.axis('off')\n",
        "        if titles is not None and ii < len(titles):\n",
        "            plt.title(str(titles[ii]))\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def plot(x_values, y_values, title, xlabel, ylabel):\n",
        "    \"\"\"Plots a line graph\n",
        "\n",
        "    Args:\n",
        "        x_values(list or np.array): x values for the line\n",
        "        y_values(list or np.array): y values for the line\n",
        "        title(str): Title for the plot\n",
        "        xlabel(str): Label for the x axis\n",
        "        ylabel(str): label for the y axis\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(x_values, y_values)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "def to_scaled_uint8(array):\n",
        "    \"\"\"Returns a normalized uint8 scaled to 0-255. This is useful for showing images especially of floats.\n",
        "\n",
        "    Args:\n",
        "        array(np.array): The array to normalize\n",
        "    Returns:\n",
        "        np.array normalized and of type uint8\n",
        "    \"\"\"\n",
        "\n",
        "    array = np.array(array, dtype=np.float32)\n",
        "    array -= np.min(array)\n",
        "    array *= (255. / np.max(array))\n",
        "    array = array.astype(np.uint8)\n",
        "    return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrz6eOudXWHY"
      },
      "source": [
        "# The functions in this cell are from CSE 543 Deep Learning.\n",
        "def save_model(model, file_path, num_to_keep=1):\n",
        "    pt_util.save(model, file_path, num_to_keep)\n",
        "\n",
        "def load_model(model, file_path):\n",
        "    pt_util.restore(model, file_path)\n",
        "\n",
        "def load_last_model(model, dir_path):\n",
        "    return pt_util.restore_latest(model, dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6svkSDr6vsy8"
      },
      "source": [
        "## Dataset class for the [Berkeley DeepDrive Dataset](https://arxiv.org/abs/1805.04687)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfJGn92hvu9Q"
      },
      "source": [
        "class DeepDriveDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_data):\n",
        "        super(DeepDriveDataset, self).__init__()\n",
        "\n",
        "        self.image_data = image_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        cur_image = cv2.cvtColor(cv2.imread(self.image_data[idx][0]), cv2.COLOR_BGR2RGB)\n",
        "        return (transforms.ToTensor()(cur_image), self.image_data[idx][1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8_sdIOZfi77"
      },
      "source": [
        "## Clone the TorchVision repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnyv8sPgIkPr"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Only run this cell the very first time you run the notebook\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCMmZmuaO3P_"
      },
      "source": [
        "## Load the training, val, and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hf0NapJO44e"
      },
      "source": [
        "train_val_test_data = dict()\n",
        "# Edit the file path below to go to the location of the labels file for Faster R-CNN\n",
        "with open('/content/drive/MyDrive/CV_Project/faster_rcnn_labels.p', 'rb') as images_file:\n",
        "    train_val_test_data = pickle.load(images_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDxWBa0NvQ0n"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtyfBO4pCQ2k"
      },
      "source": [
        "# All the code in this cell, with some modifications by me (namely, the learning\n",
        "# rate scheduler code and returning the total loss in train_one_epoch), is from\n",
        "# https://github.com/pytorch/vision/blob/master/references/detection/engine.py\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "import utils\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, lr_scheduler, lr_change_thresh, data_loader, device, epoch, print_freq):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    total_loss = 0\n",
        "    prev_loss = None\n",
        "    loss_increases = 0\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "        total_loss += loss_value\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if prev_loss is None:\n",
        "            prev_loss = loss_value\n",
        "        elif loss_value < prev_loss:\n",
        "            loss_increases = 0\n",
        "            prev_loss = loss_value\n",
        "        else:\n",
        "            loss_increases += 1\n",
        "            prev_loss = loss_value\n",
        "            if loss_increases >= lr_change_thresh:\n",
        "                lr_scheduler.step()\n",
        "                loss_increases = 0\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = [\"bbox\"]\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yYurf77vVXo"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 9\n",
        "VAL_BATCH_SIZE = 5\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.00005\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "LR_CHANGE_THRESH = 4\n",
        "GAMMA = 0.95\n",
        "\n",
        "# The lines below, up to creating kwargs, as well as the except and finally\n",
        "# blocks at the end of this cell are from CSE 543 Deep Learning\n",
        "# (https://github.com/pjreddie/uwnet/blob/master/hw1.ipynb).\n",
        "\n",
        "EXPERIMENT_VERSION = '0.001' # increment this to start a new experiment\n",
        "LOG_PATH = BASE_PATH + 'faster_rcnn_logs/' + EXPERIMENT_VERSION + '/'\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "data_train = DeepDriveDataset(train_val_test_data['train_labels'])\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, collate_fn=utils.collate_fn)\n",
        "\n",
        "# The code below for loading the model, creating the learning rate scheduler, and calling\n",
        "# train_one_epoch and evaluate are from the PyTorch TorchVision Object Detection Fine\n",
        "# Tuning Tutorial (https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
        "# To fine tune Faster R-CNN with a MobileNetV3 backbone instead of ResNet-50, comment\n",
        "# out the first line below and uncomment the second line below.\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "num_classes = 14\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=GAMMA)\n",
        "start_epoch = load_last_model(model, LOG_PATH)\n",
        "\n",
        "train_losses, val_losses, val_mAPs = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))\n",
        "max_val_mAP = None\n",
        "if len(val_mAPs) > 0:\n",
        "    max_val_mAP = val_mAPs[len(val_mAPs) - 1][1]\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        # Train the model for one epoch\n",
        "        train_loss = train_one_epoch(model, optimizer, lr_scheduler, LR_CHANGE_THRESH, train_loader, device, epoch, print_freq=10)\n",
        "        data_val_for_loss = DeepDriveDataset(copy.deepcopy(train_val_test_data['val_labels']))\n",
        "        val_loader_for_loss = torch.utils.data.DataLoader(data_val_for_loss, batch_size=VAL_BATCH_SIZE,\n",
        "                                                          shuffle=False, collate_fn=utils.collate_fn)\n",
        "        # Compute loss on the validation set\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, targets) in enumerate(val_loader_for_loss):\n",
        "                cur_images = list(image.to(device) for image in images)\n",
        "                cur_targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "                loss_dict = model(cur_images, cur_targets)\n",
        "                cur_loss = sum(loss for loss in loss_dict.values())\n",
        "                val_loss += cur_loss.item()\n",
        "        \n",
        "        # Evaluate the model on the validation set. It is important to make a copy\n",
        "        # of the data passed to the evaluate function as this function appears to\n",
        "        # modify the data in a way that can make the bounding boxes invalid for\n",
        "        # passing to the model.\n",
        "        data_val_for_evaluate = DeepDriveDataset(copy.deepcopy(train_val_test_data['val_labels']))\n",
        "        val_loader_for_evaluate = torch.utils.data.DataLoader(data_val_for_evaluate, batch_size=VAL_BATCH_SIZE,\n",
        "                                                              shuffle=False, collate_fn=utils.collate_fn)\n",
        "        val_evaluator = evaluate(model, val_loader_for_evaluate, device=device)\n",
        "        \n",
        "        print('Total loss on validation set:', val_loss)\n",
        "        \n",
        "        val_mAP = val_evaluator.coco_eval['bbox'].stats[0]\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        val_losses.append((epoch, val_loss))\n",
        "        val_mAPs.append((epoch, val_mAP))\n",
        "        pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, val_losses, val_mAPs))\n",
        "        if max_val_mAP is None or val_mAP > max_val_mAP:\n",
        "            save_model(model, LOG_PATH + '%03d.pt' % epoch, 1)\n",
        "            max_val_mAP = val_mAP\n",
        "        print('Current learning rate:', optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    save_model(model, LOG_PATH + '%03d.pt' % epoch, 0)\n",
        "    ep, val = zip(*train_losses)\n",
        "    pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Loss')\n",
        "    ep, val = zip(*val_losses)\n",
        "    pt_util.plot(ep, val, 'Val loss', 'Epoch', 'Loss')\n",
        "    ep, val = zip(*val_mAPs)\n",
        "    pt_util.plot(ep, val, 'Val mAP', 'Epoch', 'mAP')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zaeFLeJuXYD"
      },
      "source": [
        "## Evaluate the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94eA0xeCjzzh"
      },
      "source": [
        "# Change 'faster-rcnn-resnet-exp-4' to the name of the folder containing\n",
        "# the logs for the model to evaluate\n",
        "LOG_PATH = BASE_PATH + 'faster-rcnn-resnet-exp-4/'\n",
        "# Comment out the first line below and uncomment the second line below if\n",
        "# the model you want to evaluate has a MobileNetV3 backbone.\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "num_classes = 14\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "start_epoch = load_last_model(model, LOG_PATH)\n",
        "\n",
        "train_losses, val_losses, val_mAPs = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86UcrwaEkPvk"
      },
      "source": [
        "data_test = DeepDriveDataset(copy.deepcopy(train_val_test_data['test_labels']))\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=5,\n",
        "                                          shuffle=False, collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miOMpyXzkQTs"
      },
      "source": [
        "test_evaluator = evaluate(model, test_loader, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOI6jbiXkSf-"
      },
      "source": [
        "### Compute mAP on the test set for each object class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GblPtTKEkWeu"
      },
      "source": [
        "for i in range(0, num_classes):\n",
        "    print('Computing mAP for class', i)\n",
        "    cur_class_test_data = []\n",
        "    for test_image, test_label in train_val_test_data['test_labels']:\n",
        "        cur_boxes = torch.tensor([])\n",
        "        cur_area = torch.tensor([])\n",
        "        for j in range(0, test_label['labels'].size(0)):\n",
        "            if test_label['labels'][j].item() == i:\n",
        "                cur_boxes = torch.cat((cur_boxes, torch.unsqueeze(test_label['boxes'][j], 0)), 0)\n",
        "                cur_area = torch.cat((cur_area, torch.unsqueeze(test_label['area'][j], 0)), 0)\n",
        "        num_boxes = cur_boxes.size()[0]\n",
        "        if num_boxes > 0:\n",
        "            cur_label = {\n",
        "                'image_id': test_label['image_id'],\n",
        "                'boxes': cur_boxes,\n",
        "                'area': cur_area,\n",
        "                'labels': torch.zeros((num_boxes,), dtype=torch.int64).fill_(i),\n",
        "                'iscrowd': torch.zeros((num_boxes,), dtype=torch.int64)\n",
        "            }\n",
        "            cur_class_test_data.append((test_image, cur_label))\n",
        "    print(len(cur_class_test_data), 'images for class', i)\n",
        "    if len(cur_class_test_data) > 0:\n",
        "        print('Finished creating dataset, starting evaluation')\n",
        "        cur_data_test = DeepDriveDataset(copy.deepcopy(cur_class_test_data))\n",
        "        cur_test_loader = torch.utils.data.DataLoader(cur_data_test, batch_size=5,\n",
        "                                                      shuffle=False, collate_fn=utils.collate_fn)\n",
        "        cur_evaluator = evaluate(model, cur_test_loader, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}